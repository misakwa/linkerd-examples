################################################################################
# Linkerd Service Mesh
#
# This is a basic Kubernetes config file to deploy a service mesh of Linkerd
# instances onto your Kubernetes cluster that is capable of handling HTTP,
# HTTP/2 and gRPC calls with some reasonable defaults.
#
# To configure your applications to use Linkerd for HTTP traffic you can set the
# `http_proxy` environment variable to `$(NODE_NAME):4140` where `NODE_NAME` is
# the name of node on which the application instance is running.  The
# `NODE_NAME` environment variable can be set with the downward API.
#
# If your application does not support the `http_proxy` environment variable or
# if you want to configure your application to use Linkerd for HTTP/2 or gRPC
# traffic, you must configure your application to send traffic directly to
# Linkerd:
#
# * $(NODE_NAME):4140 for HTTP
# * $(NODE_NAME):4240 for HTTP/2
# * $(NODE_NAME):4340 for gRPC
#
# If you are sending HTTP or HTTP/2 traffic directly to Linkerd, you must set
# the Host/Authority header to `<service>` or `<service>.<namespace>` where
# `<service>` and `<namespace>` are the names of the service and namespace
# that you want to proxy to.  If unspecified, `<namespace>` defaults to
# `default`.
#
# If your application receives HTTP, HTTP/2, and/or gRPC traffic it must have a
# Kubernetes Service object with ports named `http`, `h2`, and/or `grpc`
# respectively.
#
# You can deploy this to your Kubernetes cluster by running:
#   kubectl create ns linkerd
#   kubectl apply -n linkerd -f servicemesh.yml
#
# There are sections of this config that can be uncommented to enable:
# * CNI compatibility
# * Automatic retries
# * Zipkin tracing
################################################################################
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: l5d-config
  namespace: linkerd
data:
  config.yaml: |-
    admin:
      ip: 0.0.0.0
      port: 9990

    # Telemeters export metrics and tracing data about Linkerd, the services it
    # connects to, and the requests it processes.
    telemetry:
    - kind: io.l5d.prometheus # Expose Prometheus style metrics on :9990/admin/metrics/prometheus
    - kind: io.l5d.recentRequests
      sampleRate: 0.005 # Tune this sample rate before going to production
    # - kind: io.l5d.zipkin # Uncomment to enable exporting of zipkin traces
    #   host: zipkin-collector.default.svc.cluster.local # Zipkin collector address
    #   port: 9410
    #   sampleRate: 1.0 # Set to a lower sample rate depending on your traffic volume

    # Usage is used for anonymized usage reporting.  You can set the orgId to
    # identify your organization or set `enabled: false` to disable entirely.
    usage:
      enabled: false
      orgId: linkerd-examples-servicemesh

    # Routers define how Linkerd actually handles traffic.  Each router listens
    # for requests, applies routing rules to those requests, and proxies them
    # to the appropriate destinations.  Each router is protocol specific.
    # For each protocol (HTTP, HTTP/2, gRPC) we define an outgoing router and
    # an incoming router.  The application is expected to send traffic to the
    # outgoing router which proxies it to the incoming router of the Linkerd
    # running on the target service's node.  The incoming router then proxies
    # the request to the target application itself.  We also define HTTP and
    # HTTP/2 ingress routers which act as Ingress Controllers and route based
    # on the Ingress resource.
    routers:
    - label: http-outgoing
      protocol: http
      originator: true
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: http-outgoing
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.http.retryableIdempotent5XX
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true
      servers:
      - port: 4140
        ip: 0.0.0.0

    - label: http-incoming
      protocol: http
      servers:
      - port: 4141
        ip: 0.0.0.0
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: http-incoming
        # hostNetwork: true
        transformers:
        - kind: io.l5d.k8s.localnode
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.http.retryableIdempotent5XX
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

    - label: h2-outgoing
      protocol: h2
      originator: true
      servers:
      - port: 4240
        ip: 0.0.0.0
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: h2-outgoing
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.h2.retryableIdempotent5XX
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

    - label: h2-incoming
      protocol: h2
      servers:
      - port: 4241
        ip: 0.0.0.0
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: h2-incoming
        # hostNetwork: true
        transformers:
        - kind: io.l5d.k8s.localnode
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.h2.retryableIdempotent5XX
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

    - label: grpc-outgoing
      protocol: h2
      originator: true
      identifier:
      - kind: io.l5d.header.token
        header: x-l5d-svc
      servers:
      - port: 4340
        ip: 0.0.0.0
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: grpc-outgoing
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.h2.grpc.compliant
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

    - label: grpc-incoming
      protocol: h2
      identifier:
      - kind: io.l5d.header.token
        header: x-l5d-svc
      servers:
      - port: 4341
        ip: 0.0.0.0
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: grpc-incoming
        # hostNetwork: true
        transformers:
        - kind: io.l5d.k8s.localnode
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.h2.grpc.compliant
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

    # HTTP Ingress Controller listening on port 80
    - protocol: http
      label: http-ingress
      servers:
        - port: 80
          ip: 0.0.0.0
          clearContext: true
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: http-ingress
        # hostNetwork: true
      identifier:
        kind: io.l5d.ingress
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.http.retryableIdempotent5XX
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

    # HTTP/2 Ingress Controller listening on port 8080
    - protocol: h2
      label: h2-ingress
      servers:
        - port: 8080
          ip: 0.0.0.0
          clearContext: true
      interpreter:
        kind: io.l5d.namerd
        dst: /$/inet/namerd.linkerd.svc.cluster.local/4100
        namespace: h2-ingress
        # hostNetwork: true
      identifier:
        kind: io.l5d.ingress
      service:
        retries:
          budget:
            percentCanRetry: 0.3
      #   responseClassifier:
      #     kind: io.l5d.h2.retryableIdempotent5XX
      client:
        failureAccrual:
          kind: io.l5d.successRateWindowed
          successRate: 0.9
          window: 300
        loadBalancer:
          kind: ewma
          enableProbation: true

---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    app: l5d
  name: l5d
  namespace: linkerd
spec:
  minReadySeconds: 30
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: l5d
    spec:
      # hostNetwork: true # Uncomment to use host networking (eg for CNI)
      volumes:
      - name: l5d-config
        configMap:
          name: "l5d-config"
      containers:
      - name: l5d
        image: buoyantio/linkerd:1.6.4
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        args:
        - /io.buoyant/linkerd/config/config.yaml
        ports:
        - name: http-outgoing
          containerPort: 4140
          hostPort: 4140
        - name: http-incoming
          containerPort: 4141
        - name: h2-outgoing
          containerPort: 4240
          hostPort: 4240
        - name: h2-incoming
          containerPort: 4241
        - name: grpc-outgoing
          containerPort: 4340
          hostPort: 4340
        - name: grpc-incoming
          containerPort: 4341
        - name: http-ingress
          containerPort: 80
        - name: h2-ingress
          containerPort: 8080
        - name: admin
          containerPort: 9990
        livenessProbe:
          httpGet:
            path: /admin/ping
            port: 9990
          initialDelaySeconds: 10
        volumeMounts:
        - name: "l5d-config"
          mountPath: "/io.buoyant/linkerd/config"
          readOnly: true

      # Run `kubectl proxy` as a sidecar to give us authenticated access to the
      # Kubernetes API.
      - name: kubectl
        image: buoyantio/kubectl:v1.14.3
        args:
        - "proxy"
        - "-p"
        - "8001"

---
apiVersion: v1
kind: Service
metadata:
  name: l5d
  namespace: linkerd
spec:
  type: ClusterIP
  selector:
    app: l5d
  ports:
  - name: http-outgoing
    port: 4140
  - name: http-incoming
    port: 4141
  - name: h2-outgoing
    port: 4240
  - name: h2-incoming
    port: 4241
  - name: grpc-outgoing
    port: 4340
  - name: grpc-incoming
    port: 4341
  - name: http-ingress
    port: 80
  - name: h2-ingress
    port: 8080
  - name: admin
    port: 9990
